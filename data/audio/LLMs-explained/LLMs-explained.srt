1
00:00:00,200 --> 00:00:03,700
have you ever been in a chat with an AI and it feels amazing

2
00:00:03,866 --> 00:00:07,133
then suddenly it forgets the most important thing you said

3
00:00:07,300 --> 00:00:09,666
like you told it make it a shortlist

4
00:00:09,666 --> 00:00:13,133
and three messages later it drops a giant essay anyway

5
00:00:13,266 --> 00:00:17,533
or you say don't mention spoilers and it casually spoils the plot

6
00:00:17,966 --> 00:00:21,633
it's tempting to think the AI is being lazy or dramatic

7
00:00:21,633 --> 00:00:22,833
but most of the time

8
00:00:22,833 --> 00:00:25,966
it's something way more boring and often very real

9
00:00:25,966 --> 00:00:29,666
it ran out of space and that space has a name

10
00:00:29,666 --> 00:00:32,900
it's the context window here's the twist

11
00:00:33,033 --> 00:00:34,800
your message becomes tokens

12
00:00:34,866 --> 00:00:36,733
the model's reply does too

13
00:00:36,866 --> 00:00:39,166
both spend the same limited budget

14
00:00:39,400 --> 00:00:43,333
once you see that AI amnesia starts making a lot more sense

15
00:00:43,766 --> 00:00:47,266
in the next few minutes I'll make two weird words feel normal

16
00:00:47,266 --> 00:00:49,133
tokens and context window

17
00:00:49,600 --> 00:00:51,500
first we'll talk about tokens

18
00:00:51,600 --> 00:00:53,200
they're not exactly words

19
00:00:53,200 --> 00:00:56,066
they're more like chunks of text the model works with

20
00:00:56,433 --> 00:00:58,866
second we'll talk about the context window

21
00:00:59,033 --> 00:01:00,900
that's the maximum amount of tokens

22
00:01:00,900 --> 00:01:03,466
the model can pay attention to at one time

23
00:01:03,966 --> 00:01:05,600
then we'll hit the main rule

24
00:01:05,666 --> 00:01:08,633
your input tokens plus the model's output tokens

25
00:01:08,633 --> 00:01:12,333
have to fit inside one shared limit in most chat setups

26
00:01:12,700 --> 00:01:15,133
and finally you'll get a simple toolkit

27
00:01:15,166 --> 00:01:17,400
you'll learn how to keep the AI on track

28
00:01:17,400 --> 00:01:20,666
even in long chats without any coding or tech jargon

29
00:01:21,233 --> 00:01:25,166
let's start with tokens because this is the part nobody explains well

30
00:01:25,400 --> 00:01:26,633
when you type a message

31
00:01:26,633 --> 00:01:29,500
the model doesn't truly see words the way you do

32
00:01:29,500 --> 00:01:33,366
it sees tokens a token is a small chunk of text

33
00:01:33,433 --> 00:01:34,966
sometimes it's a whole word

34
00:01:34,966 --> 00:01:36,666
sometimes it's part of a word

35
00:01:36,700 --> 00:01:38,033
sometimes it's punctuation

36
00:01:38,033 --> 00:01:41,866
like a comma or a dash or three exclamation points in a row

37
00:01:42,066 --> 00:01:43,833
here's a simple way to picture it

38
00:01:43,833 --> 00:01:46,133
imagine your sentence is a Lego build

39
00:01:46,200 --> 00:01:48,333
words are the big pieces you notice

40
00:01:48,366 --> 00:01:49,866
tokens are the smaller pieces

41
00:01:49,866 --> 00:01:52,000
the model snaps together behind the scenes

42
00:01:52,366 --> 00:01:54,700
so if you type something like unbelievable

43
00:01:54,700 --> 00:01:56,766
it might get split into a couple of chunks

44
00:01:56,766 --> 00:01:58,766
instead of staying as one neat piece

45
00:01:58,900 --> 00:02:01,000
and if you type wait what

46
00:02:01,033 --> 00:02:04,666
the dots and punctuation can add extra tokens you didn't expect

47
00:02:04,666 --> 00:02:06,633
that's why this looks short

48
00:02:06,633 --> 00:02:09,633
can still be expensive and this looks long

49
00:02:09,633 --> 00:02:11,266
might be cheaper than you think

50
00:02:11,500 --> 00:02:13,866
people sometimes use a rough rule of thumb

51
00:02:13,866 --> 00:02:17,600
like in English tokens average around four characters

52
00:02:17,866 --> 00:02:19,566
that's a handy approximation

53
00:02:19,566 --> 00:02:23,633
but it's not exact don't treat that like a law of physics

54
00:02:23,633 --> 00:02:26,200
it varies a lot by language spacing

55
00:02:26,200 --> 00:02:28,500
formatting and the exact text

56
00:02:29,000 --> 00:02:31,466
the only point you need today is this

57
00:02:31,600 --> 00:02:33,966
tokens are the unit the model counts

58
00:02:34,100 --> 00:02:37,600
and the model has a maximum number it can handle at once

59
00:02:38,033 --> 00:02:40,100
now let's talk about that maximum

60
00:02:40,400 --> 00:02:43,900
the context window is the amount of text in tokens

61
00:02:43,966 --> 00:02:46,233
that the model can consider at one time

62
00:02:46,233 --> 00:02:48,100
while generating its next reply

63
00:02:48,466 --> 00:02:50,600
I like to call it working memory

64
00:02:50,600 --> 00:02:54,000
but with one big warning it's not human memory

65
00:02:54,000 --> 00:02:56,400
it's more like a limited sized whiteboard

66
00:02:56,700 --> 00:02:58,966
you can write a bunch of stuff on the whiteboard

67
00:02:58,966 --> 00:03:02,600
but once it's full you can't keep adding without erasing something

68
00:03:02,866 --> 00:03:06,600
in chat the whiteboard holds the conversation so far

69
00:03:06,766 --> 00:03:08,766
that includes your earlier messages

70
00:03:08,766 --> 00:03:10,500
the model's earlier replies

71
00:03:10,500 --> 00:03:13,366
and sometimes hidden system text or instructions

72
00:03:13,866 --> 00:03:16,666
so what happens when the chat gets too long

73
00:03:16,866 --> 00:03:20,000
in some apps older messages may be truncated

74
00:03:20,000 --> 00:03:23,200
summarized or no longer fully visible to the model

75
00:03:23,366 --> 00:03:25,300
and when the model can't see something

76
00:03:25,300 --> 00:03:27,300
it can't reliably follow it

77
00:03:27,600 --> 00:03:29,600
that's when you get the classic moment

78
00:03:29,666 --> 00:03:31,066
you set a rule at the start

79
00:03:31,066 --> 00:03:33,066
like keep it under 100 words

80
00:03:33,066 --> 00:03:34,900
and later the model breaks it

81
00:03:35,000 --> 00:03:36,566
not because it wants to

82
00:03:36,566 --> 00:03:39,533
but because the rule might not be inside the window anymore

83
00:03:39,866 --> 00:03:42,200
and yes this is a real constraint

84
00:03:42,200 --> 00:03:43,866
not just bad design

85
00:03:44,000 --> 00:03:48,133
processing more context generally takes more computation in memory

86
00:03:48,433 --> 00:03:50,000
one more important note

87
00:03:50,200 --> 00:03:53,700
some apps advertise extra features like saved memory

88
00:03:53,700 --> 00:03:57,600
profile notes or retrieval from documents that can help

89
00:03:57,600 --> 00:04:00,100
but the model still generates each response

90
00:04:00,100 --> 00:04:02,633
based on the tokens it has in its active context

91
00:04:02,633 --> 00:04:07,000
right now this is the most useful idea in the whole video

92
00:04:07,000 --> 00:04:08,800
so I'm going to say it slowly

93
00:04:09,000 --> 00:04:13,600
the context window is not just what you type in many chat tools

94
00:04:13,600 --> 00:04:16,600
it's what you type plus what the model replies

95
00:04:16,866 --> 00:04:19,900
so you and the model are basically sharing one backpack

96
00:04:19,900 --> 00:04:22,100
your prompt goes in the backpack

97
00:04:22,166 --> 00:04:25,066
then the model's answer goes in the same backpack

98
00:04:25,066 --> 00:04:27,500
and the backpack has a fixed size

99
00:04:27,766 --> 00:04:31,166
that means a long answer doesn't just cost time

100
00:04:31,166 --> 00:04:34,066
it can crowd out older instructions and details

101
00:04:34,333 --> 00:04:36,500
here's a situation you've probably lived

102
00:04:36,700 --> 00:04:39,000
you paste a big wall of text and say

103
00:04:39,000 --> 00:04:42,666
read all of this and write me a detailed 2,000 word response

104
00:04:42,800 --> 00:04:44,900
the model tries it starts writing

105
00:04:44,900 --> 00:04:47,866
but the more it writes the more tokens it uses

106
00:04:47,866 --> 00:04:50,433
and those output tokens are eating into the same

107
00:04:50,433 --> 00:04:52,166
total context capacity

108
00:04:52,400 --> 00:04:55,500
so the model can end up in a weird spot where it's generating

109
00:04:55,500 --> 00:04:57,566
but it no longer has room to keep the full

110
00:04:57,566 --> 00:04:59,400
original instructions in view

111
00:04:59,766 --> 00:05:03,900
that's why a smarter move is often ask for a tight outline first

112
00:05:03,900 --> 00:05:06,200
then expand one section at a time

113
00:05:06,400 --> 00:05:07,900
you're not lowering quality

114
00:05:07,900 --> 00:05:09,366
you're managing the budget

115
00:05:09,633 --> 00:05:11,166
think of it like ordering food

116
00:05:11,166 --> 00:05:12,666
if you order everything at once

117
00:05:12,666 --> 00:05:14,500
your table gets crowded and messy

118
00:05:14,500 --> 00:05:16,766
if you order in courses you keep control

119
00:05:16,833 --> 00:05:18,766
same information better pacing

120
00:05:18,766 --> 00:05:19,766
less chaos

121
00:05:20,300 --> 00:05:23,400
now why don't we just make the context window unlimited

122
00:05:23,666 --> 00:05:26,600
because more context usually means more work

123
00:05:26,766 --> 00:05:27,633
at a high level

124
00:05:27,633 --> 00:05:31,133
the model has to consider the context while choosing each next token

125
00:05:31,233 --> 00:05:32,700
so when the context grows

126
00:05:32,700 --> 00:05:35,333
the computation needed per step can increase

127
00:05:35,600 --> 00:05:37,300
you can feel this in real tools

128
00:05:37,300 --> 00:05:40,333
replies can get slower apps may warn you about limits

129
00:05:40,366 --> 00:05:42,900
some interfaces nudge you towards shorter prompts

130
00:05:43,200 --> 00:05:46,000
also different models can offer different context sizes

131
00:05:46,000 --> 00:05:48,266
and those choices often involve trade offs

132
00:05:48,266 --> 00:05:50,733
sometimes you get a bigger window but higher cost

133
00:05:50,800 --> 00:05:53,000
sometimes you get speed but a smaller window

134
00:05:53,000 --> 00:05:54,666
and sometimes you get good enough memory

135
00:05:54,666 --> 00:05:57,500
because the app summarizes or compresses old messages

136
00:05:57,766 --> 00:06:01,633
so yes a bigger context window can improve results in many cases

137
00:06:01,633 --> 00:06:02,900
but it's not free

138
00:06:02,900 --> 00:06:05,733
it's an engineering constraint you can't just wish away

139
00:06:05,966 --> 00:06:08,933
so bigger windows help but you can still hit the ceiling

140
00:06:09,066 --> 00:06:11,700
and if you do here's what that looks like in real chats

141
00:06:12,066 --> 00:06:14,333
okay what actually happens when you hit the limit

142
00:06:14,433 --> 00:06:16,366
a few common things sometimes

143
00:06:16,366 --> 00:06:19,600
the tool refuses your request and tells you the context is too long

144
00:06:19,766 --> 00:06:23,166
sometimes it silently drops or summarizes older parts of the chat

145
00:06:23,233 --> 00:06:26,400
sometimes it produces an answer that sounds confident

146
00:06:26,400 --> 00:06:29,900
but ignores earlier rules or facts you already gave it

147
00:06:30,100 --> 00:06:32,700
and that last one is the most confusing

148
00:06:32,766 --> 00:06:35,366
because it feels like the model is lying

149
00:06:35,366 --> 00:06:39,700
but a simpler explanation is often it can't see what it needs anymore

150
00:06:40,266 --> 00:06:43,133
here's a practical diagnostic you can use today

151
00:06:43,200 --> 00:06:46,166
when the model starts contradicting your earlier constraints

152
00:06:46,366 --> 00:06:46,833
first

153
00:06:46,833 --> 00:06:50,466
suspect that those constraints aren't in its active context anymore

154
00:06:50,766 --> 00:06:52,366
that's not you being paranoid

155
00:06:52,433 --> 00:06:55,300
that's you being realistic about a limited window

156
00:06:55,866 --> 00:06:58,400
so if that's what hitting the ceiling looks like

157
00:06:58,400 --> 00:07:00,266
what can you actually do about it

158
00:07:00,466 --> 00:07:04,200
now let's turn all of this into habits you can actually use

159
00:07:04,633 --> 00:07:06,600
first work in stages

160
00:07:06,766 --> 00:07:09,700
ask for an outline then pick one section

161
00:07:09,866 --> 00:07:15,066
then expand it this keeps each turn smaller and easier to keep in view

162
00:07:15,500 --> 00:07:18,066
second keep your instructions compact

163
00:07:18,200 --> 00:07:20,600
instead of writing a long story about what you want

164
00:07:20,600 --> 00:07:24,866
try a tiny constraints block like tone friendly format

165
00:07:24,866 --> 00:07:27,266
bullets must include three examples

166
00:07:27,266 --> 00:07:28,733
must avoid spoilers

167
00:07:29,266 --> 00:07:32,166
third refresh the context on purpose

168
00:07:32,266 --> 00:07:36,033
every few turns ask the model to summarize the key facts and decisions

169
00:07:36,033 --> 00:07:37,466
in five to eight bullets

170
00:07:37,600 --> 00:07:40,700
then paste that summary back into the chat when you continue

171
00:07:40,800 --> 00:07:43,233
that's like pinning the important notes onto the whiteboard

172
00:07:43,233 --> 00:07:44,533
so they don't get erased

173
00:07:45,066 --> 00:07:47,100
fourth don't paste everything

174
00:07:47,233 --> 00:07:50,466
paste what matters if you're asking about one paragraph

175
00:07:50,466 --> 00:07:53,200
share one paragraph if you're asking about one scene

176
00:07:53,200 --> 00:07:56,466
share one scene you'll usually get better results with smaller

177
00:07:56,466 --> 00:08:00,200
cleaner inputs so next time an AI forgets

178
00:08:00,200 --> 00:08:02,700
don't just get annoyed get strategic

179
00:08:03,100 --> 00:08:05,700
remember the hidden rule in many chat tools

180
00:08:05,700 --> 00:08:08,766
you're spending tokens on the way in and on the way out

181
00:08:08,766 --> 00:08:10,933
inside one fixed context window

182
00:08:11,466 --> 00:08:13,966
try this the very next time you use a chat tool

183
00:08:14,033 --> 00:08:17,233
ask for a short outline then expand one part at a time

184
00:08:17,233 --> 00:08:19,500
keep it running bullet summary and paste it forward

185
00:08:19,966 --> 00:08:21,100
now I'm curious

186
00:08:21,100 --> 00:08:24,100
what's the biggest thing you've tried to do in a single prompt

187
00:08:24,166 --> 00:08:25,566
was it rewriting a document

188
00:08:25,566 --> 00:08:27,700
planning a project studying for an exam

189
00:08:27,766 --> 00:08:28,633
tell me in the comments

190
00:08:28,633 --> 00:08:31,466
because I read them and I borrow your ideas for future videos

191
00:08:31,866 --> 00:08:35,300
and if you want simple explanations of how these tools actually work

192
00:08:35,300 --> 00:08:36,400
tokens context

193
00:08:36,400 --> 00:08:37,733
windows hallucinations

194
00:08:37,800 --> 00:08:39,700
and how to get more reliable answers

195
00:08:39,700 --> 00:08:40,666
hit subscribe

196
00:08:40,666 --> 00:08:44,466
I've got more videos coming to help you get better results with LMS

197
00:08:44,466 --> 00:08:46,700
even if you never write a line of code

