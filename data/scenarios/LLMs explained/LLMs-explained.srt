1
00:00:00,100 --> 00:00:03,866
have you ever been in a chat with an AI and it feels amazing

2
00:00:03,866 --> 00:00:06,966
then suddenly it forgets the most important thing you said

3
00:00:07,033 --> 00:00:10,100
like you told it make it a shortlist

4
00:00:10,200 --> 00:00:14,133
and three messages later it drops a giant essay anyway

5
00:00:14,166 --> 00:00:19,100
or you say don't mention spoilers and it casually spoils the plot

6
00:00:19,300 --> 00:00:23,066
it's tempting to think the AI is being lazy or dramatic

7
00:00:23,200 --> 00:00:24,566
but most of the time

8
00:00:24,566 --> 00:00:27,433
it's something way more boring and often very real

9
00:00:27,433 --> 00:00:30,900
it ran out of space and that space has a name

10
00:00:30,900 --> 00:00:34,000
it's the context window here's the twist

11
00:00:34,100 --> 00:00:36,000
your message becomes tokens

12
00:00:36,266 --> 00:00:38,333
the models replied does too

13
00:00:38,400 --> 00:00:40,866
both spend the same limited budget

14
00:00:41,066 --> 00:00:45,466
once you see that AI amnesia starts making a lot more sense

15
00:00:45,500 --> 00:00:49,300
in the next few minutes I'll make two weird words feel normal

16
00:00:49,300 --> 00:00:51,266
tokens and context window

17
00:00:51,433 --> 00:00:53,733
first we'll talk about tokens

18
00:00:54,033 --> 00:00:55,866
they're not exactly words

19
00:00:56,066 --> 00:00:59,133
they're more like chunks of text the model works with

20
00:00:59,233 --> 00:01:02,200
second we'll talk about the context window

21
00:01:02,433 --> 00:01:04,466
that's the maximum amount of tokens

22
00:01:04,466 --> 00:01:07,200
the model can pay attention to at one time

23
00:01:07,266 --> 00:01:09,066
then we'll hit the main rule

24
00:01:09,233 --> 00:01:12,233
your input tokens plus the model's output tokens

25
00:01:12,233 --> 00:01:15,766
have to fit inside one shared limit in most chat setups

26
00:01:15,966 --> 00:01:18,800
and finally you'll get a simple toolkit

27
00:01:19,066 --> 00:01:21,400
you'll learn how to keep the AI on track

28
00:01:21,400 --> 00:01:25,000
even in long chats without any coding or tech jargon

29
00:01:25,200 --> 00:01:29,733
let's start with tokens because this is the part nobody explains well

30
00:01:29,800 --> 00:01:31,266
when you type a message

31
00:01:31,266 --> 00:01:34,366
the model doesn't truly see words the way you do

32
00:01:34,366 --> 00:01:38,533
it sees tokens a token is a small chunk of text

33
00:01:38,800 --> 00:01:40,700
sometimes it's a whole word

34
00:01:40,900 --> 00:01:42,933
sometimes it's part of a word

35
00:01:43,200 --> 00:01:44,866
sometimes it's punctuation

36
00:01:44,866 --> 00:01:49,000
like a comma or a dash or three exclamation points in a row

37
00:01:49,166 --> 00:01:51,400
here's a simple way to picture it

38
00:01:51,466 --> 00:01:54,100
imagine your sentence is a Lego build

39
00:01:54,300 --> 00:01:56,600
words are the big pieces you notice

40
00:01:56,866 --> 00:01:58,800
tokens are the smaller pieces

41
00:01:58,800 --> 00:02:01,266
the model snaps together behind the scenes

42
00:02:01,566 --> 00:02:03,966
so if you type something like unbelievable

43
00:02:03,966 --> 00:02:06,266
it might get split into a couple of chunks

44
00:02:06,266 --> 00:02:08,333
instead of staying as one neat piece

45
00:02:08,466 --> 00:02:10,633
and if you type wait what

46
00:02:10,633 --> 00:02:14,500
the dots and punctuation can add extra tokens you didn't expect

47
00:02:14,666 --> 00:02:16,266
that's why this looks short

48
00:02:16,266 --> 00:02:19,066
can still be expensive and this looks long

49
00:02:19,066 --> 00:02:20,933
might be cheaper than you think

50
00:02:21,033 --> 00:02:23,233
people sometimes use a rough rule of thumb

51
00:02:23,233 --> 00:02:26,666
like in English tokens average around four characters

52
00:02:26,900 --> 00:02:28,800
that's a handy approximation

53
00:02:28,800 --> 00:02:32,766
but it's not exact don't treat that like a law of physics

54
00:02:33,000 --> 00:02:35,866
it varies a lot by language spacing

55
00:02:35,866 --> 00:02:38,200
formatting and the exact text

56
00:02:38,366 --> 00:02:40,766
the only point you need today is this

57
00:02:40,766 --> 00:02:43,100
tokens are the unit the model counts

58
00:02:43,266 --> 00:02:47,000
and the model has a maximum number it can handle at once

59
00:02:47,166 --> 00:02:49,500
now let's talk about that maximum

60
00:02:49,633 --> 00:02:53,000
the context window is the amount of text in tokens

61
00:02:53,000 --> 00:02:55,233
that the model can consider at one time

62
00:02:55,233 --> 00:02:57,166
while generating its next reply

63
00:02:57,266 --> 00:02:59,266
I like to call it working memory

64
00:02:59,266 --> 00:03:02,400
but with one big warning it's not human memory

65
00:03:02,433 --> 00:03:05,066
it's more like a limited size whiteboard

66
00:03:05,200 --> 00:03:07,800
you can write a bunch of stuff on the whiteboard

67
00:03:07,900 --> 00:03:12,066
but once it's full you can't keep adding without erasing something

68
00:03:12,100 --> 00:03:15,933
in chat the whiteboard holds the conversation so far

69
00:03:15,966 --> 00:03:18,100
that includes your earlier messages

70
00:03:18,100 --> 00:03:19,866
the model's earlier replies

71
00:03:19,900 --> 00:03:23,066
and sometimes hidden system text or instructions

72
00:03:23,300 --> 00:03:26,200
so what happens when the chat gets too long

73
00:03:26,266 --> 00:03:29,500
in some apps older messages may be truncated

74
00:03:29,500 --> 00:03:32,800
summarised or no longer fully visible to the model

75
00:03:32,900 --> 00:03:35,033
and when the model can't see something

76
00:03:35,033 --> 00:03:37,200
it can't reliably follow it

77
00:03:37,300 --> 00:03:39,400
that's when you get the classic moment

78
00:03:39,400 --> 00:03:41,066
you set a rule at the start

79
00:03:41,066 --> 00:03:43,033
like keep it under 100 words

80
00:03:43,033 --> 00:03:44,966
and later the model breaks it

81
00:03:45,033 --> 00:03:46,933
not because it wants to

82
00:03:47,000 --> 00:03:50,466
but because the rule might not be inside the window anymore

83
00:03:50,566 --> 00:03:53,000
and yes this is a real constraint

84
00:03:53,000 --> 00:03:54,566
not just bad design

85
00:03:54,766 --> 00:03:59,200
processing more context generally takes more computation and memory

86
00:03:59,266 --> 00:04:00,900
one more important note

87
00:04:01,000 --> 00:04:04,233
some apps advertise extra features like saved memory

88
00:04:04,233 --> 00:04:08,566
profile notes or retrieval from documents that can help

89
00:04:08,600 --> 00:04:11,100
but the model still generates each response

90
00:04:11,100 --> 00:04:13,833
based on the tokens it has in its active context

91
00:04:13,833 --> 00:04:17,833
right now this is the most useful idea in the whole video

92
00:04:17,833 --> 00:04:19,700
so I'm going to say it slowly

93
00:04:19,766 --> 00:04:24,266
the context window is not just what you type in many chat tools

94
00:04:24,266 --> 00:04:26,900
it's what you type plus what the model replies

95
00:04:27,100 --> 00:04:30,533
so you and the model are basically sharing one backpack

96
00:04:30,766 --> 00:04:32,800
your prompt goes in the backpack

97
00:04:33,033 --> 00:04:35,966
then the model's answer goes in the same backpack

98
00:04:36,100 --> 00:04:38,366
and the backpack has a fixed size

99
00:04:38,600 --> 00:04:41,666
that means a long answer doesn't just cost time

100
00:04:41,700 --> 00:04:44,900
it can crowd out older instructions and details

101
00:04:45,166 --> 00:04:47,800
here's a situation you've probably lived

102
00:04:47,966 --> 00:04:50,300
you paste a big wall of text and say

103
00:04:50,300 --> 00:04:54,100
read all of this and write me a detailed 2,000 word response

104
00:04:54,366 --> 00:04:57,133
the model tries it starts writing

105
00:04:57,466 --> 00:05:00,666
but the more it writes the more tokens it uses

106
00:05:00,800 --> 00:05:03,600
and those output tokens are eating into the same

107
00:05:03,600 --> 00:05:05,500
total context capacity

108
00:05:05,600 --> 00:05:08,966
so the model can end up in a weird spot where it's generating

109
00:05:09,033 --> 00:05:11,200
but it no longer has room to keep the full

110
00:05:11,200 --> 00:05:13,100
original instructions in view

111
00:05:13,200 --> 00:05:17,666
that's why a smarter move is often ask for a tight outline first

112
00:05:17,766 --> 00:05:20,466
then expand one section at a time

113
00:05:20,500 --> 00:05:22,400
you're not lowering quality

114
00:05:22,500 --> 00:05:24,133
you're managing the budget

115
00:05:24,400 --> 00:05:26,566
think of it like ordering food

116
00:05:26,600 --> 00:05:28,600
if you order everything at once

117
00:05:28,600 --> 00:05:30,866
your table gets crowded and messy

118
00:05:30,900 --> 00:05:33,800
if you order in courses you keep control

119
00:05:34,033 --> 00:05:36,800
same information better pacing

120
00:05:37,033 --> 00:05:39,066
less chaos now

121
00:05:39,066 --> 00:05:42,300
why don't we just make the context window unlimited

122
00:05:42,433 --> 00:05:45,533
because more context usually means more work

123
00:05:45,600 --> 00:05:46,833
at a high level

124
00:05:46,833 --> 00:05:50,766
the model has to consider the context while choosing each next token

125
00:05:50,966 --> 00:05:52,766
so when the context grows

126
00:05:52,833 --> 00:05:55,666
the computation needed per step can increase

127
00:05:55,866 --> 00:05:58,100
you can feel this in real tools

128
00:05:58,366 --> 00:06:02,366
replies can get slower apps may warn you about limits

129
00:06:02,666 --> 00:06:05,766
some interfaces nudge you towards shorter prompts

130
00:06:05,966 --> 00:06:09,700
also different models can offer different context sizes

131
00:06:09,700 --> 00:06:12,366
and those choices often involve trade offs

132
00:06:12,666 --> 00:06:15,733
sometimes you get a bigger window but higher cost

133
00:06:16,066 --> 00:06:18,900
sometimes you get speed but a smaller window

134
00:06:19,033 --> 00:06:21,166
and sometimes you get good enough memory

135
00:06:21,166 --> 00:06:24,533
because the app summarizes or compresses old messages

136
00:06:24,766 --> 00:06:29,366
so yes a bigger context window can improve results in many cases

137
00:06:29,500 --> 00:06:30,966
but it's not free

138
00:06:31,033 --> 00:06:34,533
it's an engineering constraint you can't just wish away

139
00:06:34,600 --> 00:06:38,100
so bigger windows help but you can still hit the ceiling

140
00:06:38,200 --> 00:06:41,566
and if you do here's what that looks like in real chats

141
00:06:41,900 --> 00:06:45,466
okay what actually happens when you hit the limit

142
00:06:45,633 --> 00:06:48,100
a few common things sometimes

143
00:06:48,100 --> 00:06:52,333
the tool refuses your request and tells you the context is too long

144
00:06:52,466 --> 00:06:56,933
sometimes it silently drops or summarises older parts of the chat

145
00:06:57,233 --> 00:07:00,166
sometimes it produces an answer that sounds confident

146
00:07:00,166 --> 00:07:03,533
but ignores earlier rules or facts you already gave it

147
00:07:03,600 --> 00:07:06,066
and that last one is the most confusing

148
00:07:06,066 --> 00:07:08,400
because it feels like the model is lying

149
00:07:08,600 --> 00:07:12,966
but a simpler explanation is often it can't see what it needs anymore

150
00:07:13,100 --> 00:07:15,833
here's a practical diagnostic you can use today

151
00:07:15,833 --> 00:07:18,866
when the model starts contradicting your earlier constraints

152
00:07:18,966 --> 00:07:19,300
first

153
00:07:19,300 --> 00:07:23,100
suspect that those constraints aren't in its active context anymore

154
00:07:23,200 --> 00:07:25,133
that's not you being paranoid

155
00:07:25,366 --> 00:07:28,400
that's you being realistic about a limited window

156
00:07:28,633 --> 00:07:31,266
so if that's what hitting the ceiling looks like

157
00:07:31,266 --> 00:07:33,366
what can you actually do about it

158
00:07:33,400 --> 00:07:37,266
now let's turn all of this into habits you can actually use

159
00:07:37,400 --> 00:07:39,533
first work in stages

160
00:07:39,666 --> 00:07:43,100
ask for an outline then pick one section

161
00:07:43,300 --> 00:07:48,400
then expand it this keeps each turn smaller and easier to keep in view

162
00:07:48,600 --> 00:07:51,466
second keep your instructions compact

163
00:07:51,833 --> 00:07:54,766
instead of writing a long story about what you want

164
00:07:54,766 --> 00:08:00,133
try a tiny constraints block like tone friendly format

165
00:08:00,200 --> 00:08:03,700
bullets must include three examples

166
00:08:04,000 --> 00:08:06,966
must avoid spoilers third

167
00:08:06,966 --> 00:08:09,133
refresh the context on purpose

168
00:08:09,300 --> 00:08:13,766
every few turns ask the model to summarize the key facts and decisions

169
00:08:13,766 --> 00:08:15,266
in five to eight bullets

170
00:08:15,566 --> 00:08:19,133
then paste that summary back into the chat when you continue

171
00:08:19,233 --> 00:08:22,266
that's like pinning the important notes onto the whiteboard

172
00:08:22,266 --> 00:08:24,766
so they don't get erased fourth

173
00:08:24,766 --> 00:08:27,800
don't paste everything paste what matters

174
00:08:28,033 --> 00:08:30,366
if you're asking about one paragraph

175
00:08:30,433 --> 00:08:34,133
share one paragraph if you're asking about one scene

176
00:08:34,166 --> 00:08:38,266
share one scene you'll usually get better results with smaller

177
00:08:38,266 --> 00:08:41,966
cleaner inputs so next time an AI forgets

178
00:08:42,000 --> 00:08:44,966
don't just get annoyed get strategic

179
00:08:45,366 --> 00:08:47,966
remember the hidden rule in many chat tools

180
00:08:47,966 --> 00:08:51,100
you're spending tokens on the way in and on the way out

181
00:08:51,200 --> 00:08:53,366
inside one fixed context window

182
00:08:53,500 --> 00:08:56,300
try this the very next time you use a chat tool

183
00:08:56,300 --> 00:09:00,600
ask for a short outline then expand one part at a time

184
00:09:00,666 --> 00:09:03,700
keep a running bullet summary and paste it forward

185
00:09:03,866 --> 00:09:05,300
now I'm curious

186
00:09:05,466 --> 00:09:08,866
what's the biggest thing you've tried to do in a single prompt

187
00:09:09,033 --> 00:09:11,100
was it rewriting a document

188
00:09:11,300 --> 00:09:14,700
planning a project studying for an exam

189
00:09:14,900 --> 00:09:16,300
tell me in the comments

190
00:09:16,300 --> 00:09:20,000
because I read them and I borrow your ideas for future videos

191
00:09:20,166 --> 00:09:23,700
and if you want simple explanations of how these tools actually work

192
00:09:23,700 --> 00:09:25,166
tokens context

193
00:09:25,166 --> 00:09:26,766
windows hallucinations

194
00:09:27,033 --> 00:09:29,033
and how to get more reliable answers

195
00:09:29,033 --> 00:09:30,066
hit subscribe

196
00:09:30,266 --> 00:09:34,300
I've got more videos coming to help you get better results with LLMs

197
00:09:34,300 --> 00:09:36,533
even if you never write a line of code

